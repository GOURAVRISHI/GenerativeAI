{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "openai_key = userdata.get('OPENAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# que = \"what is capital of india\"\n",
    "context = \"capital of india is new delhi and\",\" economic capital is mumbai\"\n",
    "\n",
    "answer = \"capital of india is new delhi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  create synthetic questions on give answer\n",
    "- calculate mean cosine similariy of those question with original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain openai pypdf tiktoken chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"/content/nvidia.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"db_\")\n",
    "\n",
    "retrival = db.as_retriever()\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retrival)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important terms\n",
    "\n",
    "- question is the user query.\n",
    "- answer is the llm response. \n",
    "- context is documents retrieved from the vector store/ database. \n",
    "- ground_truth is the expected answer to user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"how much money nvidia has invested in research?\",\n",
    "             \"what is expiration dates for patent?\",\n",
    "             \"what solution does nvidia offer for llms?\"\n",
    "\n",
    "]\n",
    "\n",
    "ground_truths = [[\"Nvidia have invested over $45.3 billion in research and development\"],\n",
    "                 [\"currently issued patents have expiration dates from February 2024 to August 2043\"],\n",
    "                 [\"NeMo â€“ a complete solution for building enterprise-ready Large Language Models\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "contexts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for que in questions:\n",
    "  answers.append(qa_chain.invoke(que)['result'])\n",
    "  contexts.append([doc.page_content for doc in retrival.get_relevant_documents(que)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = {\n",
    "    \"question\" : questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : ground_truths\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = Dataset.from_dict(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluating RAG application using ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import  evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate(\n",
    "    dataset = data,\n",
    "    metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
